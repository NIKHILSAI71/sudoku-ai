"""Production-grade GNN training pipeline with all optimizations.

Key Features:
- Gradient Accumulation for larger effective batch sizes
- Model EMA (Exponential Moving Average) for better generalization
- Early Stopping with patience
- Enhanced monitoring (solve rate, confidence, entropy, constraints)
- Optimized DataLoader settings
- Adaptive validation frequency
- OneCycleLR support
- Comprehensive logging

Author: Production Training Enhancement
Date: 2025-10-02
"""

from __future__ import annotations

from typing import Optional, Dict, List, Tuple
from pathlib import Path
from copy import deepcopy
from collections import OrderedDict
import logging
import time

import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR
from torch.amp.autocast_mode import autocast
from torch.amp.grad_scaler import GradScaler
from tqdm import tqdm
import numpy as np

from ..models.gnn.sudoku_gnn import SudokuGNN
from ..data.dataset import create_curriculum_dataloaders
from .loss import SudokuLoss

logger = logging.getLogger(__name__)


class ModelEMA:
    """Exponential Moving Average for model parameters.
    
    Maintains a shadow copy of model weights that are updated with EMA.
    This typically improves generalization by 0.5-2%.
    
    Args:
        model: PyTorch model to track
        decay: EMA decay rate (typical: 0.9999)
    """
    
    def __init__(self, model: nn.Module, decay: float = 0.9999):
        self.decay = decay
        self.model = model
        self.shadow = deepcopy(model)
        self.shadow.eval()
        
        # Detach shadow parameters (don't track gradients)
        for param in self.shadow.parameters():
            param.detach_()
    
    @torch.no_grad()
    def update(self, model: nn.Module):
        """Update EMA parameters.
        
        Formula: shadow = decay * shadow + (1 - decay) * model
        """
        model_params = dict(model.named_parameters())
        shadow_params = dict(self.shadow.named_parameters())
        
        assert model_params.keys() == shadow_params.keys()
        
        for name, param in model_params.items():
            shadow_params[name].mul_(self.decay).add_(param, alpha=1.0 - self.decay)
        
        # Also sync buffers (batch norm stats, etc.)
        model_buffers = dict(model.named_buffers())
        shadow_buffers = dict(self.shadow.named_buffers())
        
        for name, buffer in model_buffers.items():
            if name in shadow_buffers:
                shadow_buffers[name].copy_(buffer)
    
    def state_dict(self):
        """Get EMA state for checkpointing."""
        return {
            'decay': self.decay,
            'shadow_state_dict': self.shadow.state_dict()
        }
    
    def load_state_dict(self, state_dict):
        """Load EMA state from checkpoint."""
        self.decay = state_dict['decay']
        self.shadow.load_state_dict(state_dict['shadow_state_dict'])


class EarlyStopping:
    """Early stopping to prevent overtraining.
    
    Args:
        patience: Number of epochs to wait for improvement
        min_delta: Minimum change to qualify as improvement
        mode: 'max' for metrics to maximize (accuracy), 'min' for loss
    """
    
    def __init__(self, patience: int = 15, min_delta: float = 0.001, mode: str = 'max'):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best_score = None
        self.early_stop = False
    
    def __call__(self, score: float) -> bool:
        """Check if should stop training.
        
        Returns:
            True if should stop, False otherwise
        """
        if self.best_score is None:
            self.best_score = score
            return False
        
        if self.mode == 'max':
            is_improvement = score > self.best_score + self.min_delta
        else:
            is_improvement = score < self.best_score - self.min_delta
        
        if is_improvement:
            self.best_score = score
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
                return True
        
        return False


class ProductionGNNTrainer:
    """Production-grade trainer with all optimizations."""
    
    def __init__(
        self,
        model: SudokuGNN,
        device: str = 'cuda',
        lr: float = 1e-3,
        weight_decay: float = 1e-4,
        use_amp: bool = True,
        checkpoint_dir: str = 'checkpoints',
        gradient_clip: float = 1.0,
        warmup_epochs: int = 3,
        # Production features
        gradient_accumulation_steps: int = 2,
        use_ema: bool = True,
        ema_decay: float = 0.9999,
        early_stopping_patience: int = 15,
        scheduler_type: str = 'cosine',  # 'cosine' or 'onecycle'
        max_lr: float = 3e-3  # For OneCycleLR
    ):
        """Initialize production trainer.
        
        Args:
            model: SudokuGNN model
            device: Device to train on
            lr: Base learning rate
            weight_decay: Weight decay for regularization
            use_amp: Use automatic mixed precision
            checkpoint_dir: Directory for checkpoints
            gradient_clip: Max gradient norm
            warmup_epochs: Warmup epochs
            gradient_accumulation_steps: Steps to accumulate gradients (effective batch size multiplier)
            use_ema: Enable model EMA
            ema_decay: EMA decay rate
            early_stopping_patience: Patience for early stopping
            scheduler_type: 'cosine' or 'onecycle'
            max_lr: Max LR for OneCycleLR
        """
        self.model = model.to(device)
        self.device = device
        self.use_amp = use_amp and torch.cuda.is_available()
        self.gradient_clip = gradient_clip
        self.warmup_epochs = warmup_epochs
        self.base_lr = lr
        self.scheduler_type = scheduler_type
        self.max_lr = max_lr
        
        # Gradient accumulation
        self.accumulation_steps = gradient_accumulation_steps
        logger.info(f"Gradient accumulation steps: {self.accumulation_steps}")
        logger.info(f"Effective batch size will be: batch_size Ã— {self.accumulation_steps}")
        
        # Optimizer
        self.optimizer = AdamW(
            model.parameters(),
            lr=lr,
            weight_decay=weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # Enhanced loss function
        self.criterion = SudokuLoss(
            constraint_weight=0.5,
            use_constraint_loss=True,
            use_focal_loss=True,
            focal_gamma=2.0,
            label_smoothing=0.05
        )
        
        # Mixed precision
        self.scaler = GradScaler('cuda') if self.use_amp else None
        
        # Model EMA
        self.use_ema = use_ema
        if use_ema:
            self.ema = ModelEMA(model, decay=ema_decay)
            logger.info(f"Model EMA enabled with decay={ema_decay}")
        else:
            self.ema = None
        
        # Early stopping
        self.early_stopping = EarlyStopping(
            patience=early_stopping_patience,
            min_delta=0.001,
            mode='max'
        )
        logger.info(f"Early stopping enabled with patience={early_stopping_patience}")
        
        # Checkpointing
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # Training state
        self.best_val_acc = 0.0
        self.current_epoch = 0
        self.curriculum_stage = 0
        self.warmup_factor = 0.1
        self.training_history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_cell_acc': [],
            'val_grid_acc': [],
            'val_solve_rate': [],
            'lr': []
        }
    
    def _should_validate(self, epoch: int) -> bool:
        """Adaptive validation frequency."""
        if epoch <= 10:
            return True
        elif epoch <= 30:
            return epoch % 2 == 0
        else:
            return epoch % 3 == 0
    
    @torch.no_grad()
    def _compute_advanced_metrics(
        self,
        logits: torch.Tensor,
        solutions: torch.Tensor,
        puzzles: torch.Tensor
    ) -> Dict[str, float]:
        """Compute advanced metrics for monitoring.
        
        Returns:
            Dictionary with solve_rate, confidence, entropy, violations
        """
        probs = torch.softmax(logits, dim=-1)
        preds = logits.argmax(dim=-1) + 1  # Convert to 1-9
        mask = (puzzles == 0)
        
        # Grid solve rate
        grids_solved = 0
        total_grids = len(puzzles)
        
        for i in range(total_grids):
            if mask[i].any():
                all_correct = (preds[i][mask[i]] == solutions[i][mask[i]]).all()
                if all_correct:
                    grids_solved += 1
        
        solve_rate = grids_solved / total_grids
        
        # Confidence (mean max probability on empty cells)
        confidence = probs.max(dim=-1).values[mask].mean().item()
        
        # Entropy (uncertainty on empty cells)
        entropy = -(probs * torch.log(probs + 1e-8)).sum(dim=-1)[mask].mean().item()
        
        # Constraint violations (simplified: count duplicates in rows/cols/boxes)
        violations = self._count_constraint_violations(preds).item()
        
        return {
            'solve_rate': solve_rate,
            'confidence': confidence,
            'entropy': entropy,
            'violations': violations / total_grids  # Average per grid
        }
    
    @torch.no_grad()
    def _count_constraint_violations(self, grids: torch.Tensor) -> torch.Tensor:
        """Count Sudoku constraint violations."""
        batch_size = grids.shape[0]
        violations = torch.zeros(batch_size, device=grids.device)
        
        for i in range(batch_size):
            grid = grids[i]
            
            # Check rows
            for row in range(9):
                values = grid[row]
                unique = len(torch.unique(values[values > 0]))
                count = (values > 0).sum().item()
                violations[i] += count - unique
            
            # Check columns
            for col in range(9):
                values = grid[:, col]
                unique = len(torch.unique(values[values > 0]))
                count = (values > 0).sum().item()
                violations[i] += count - unique
            
            # Check 3x3 boxes
            for box_row in range(3):
                for box_col in range(3):
                    values = grid[box_row*3:(box_row+1)*3, box_col*3:(box_col+1)*3].flatten()
                    unique = len(torch.unique(values[values > 0]))
                    count = (values > 0).sum().item()
                    violations[i] += count - unique
        
        return violations.sum()
    
    def train_epoch(
        self,
        train_loader,
        desc: str = "Training"
    ) -> Dict[str, float]:
        """Train for one epoch with gradient accumulation.
        
        Returns:
            Dictionary with training metrics
        """
        self.model.train()
        
        total_loss = 0.0
        total_ce_loss = 0.0
        total_constraint_loss = 0.0
        correct_cells = 0
        total_cells = 0
        
        start_time = time.time()
        
        pbar = tqdm(train_loader, desc=desc)
        for batch_idx, (puzzles, solutions) in enumerate(pbar):
            puzzles = puzzles.to(self.device)
            solutions = solutions.to(self.device)
            
            # Forward pass with mixed precision
            if self.use_amp and self.scaler is not None:
                with autocast(device_type='cuda', dtype=torch.float16):
                    logits = self.model(puzzles)
                    loss, loss_info = self.criterion(logits, solutions, puzzles)
                    loss = loss / self.accumulation_steps  # Normalize for accumulation
                
                # Backward pass
                self.scaler.scale(loss).backward()
                
                # Update weights every accumulation_steps
                if (batch_idx + 1) % self.accumulation_steps == 0:
                    # Gradient clipping
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)
                    
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                    self.optimizer.zero_grad()
                    
                    # Update EMA
                    if self.ema is not None:
                        self.ema.update(self.model)
            else:
                logits = self.model(puzzles)
                loss, loss_info = self.criterion(logits, solutions, puzzles)
                loss = loss / self.accumulation_steps
                loss.backward()
                
                if (batch_idx + 1) % self.accumulation_steps == 0:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)
                    self.optimizer.step()
                    self.optimizer.zero_grad()
                    
                    if self.ema is not None:
                        self.ema.update(self.model)
            
            # Accumulate metrics
            total_loss += loss_info['total_loss']
            total_ce_loss += loss_info['ce_loss']
            total_constraint_loss += loss_info['constraint_loss']
            
            # Compute accuracy
            mask = (puzzles == 0)
            if mask.any():
                preds = logits.argmax(dim=-1) + 1
                correct = (preds[mask] == solutions[mask]).sum().item()
                total = mask.sum().item()
                correct_cells += correct
                total_cells += total
                
                acc = 100.0 * correct_cells / total_cells
                pbar.set_postfix({
                    'loss': f'{total_loss/(batch_idx+1):.4f}',
                    'acc': f'{acc:.2f}%',
                    'it/s': f'{(batch_idx+1)/(time.time()-start_time):.2f}'
                })
        
        # Handle remaining gradients
        if len(train_loader) % self.accumulation_steps != 0:
            if self.use_amp and self.scaler is not None:
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)
                self.optimizer.step()
            
            self.optimizer.zero_grad()
            if self.ema is not None:
                self.ema.update(self.model)
        
        # Average metrics
        num_batches = len(train_loader)
        epoch_time = time.time() - start_time
        
        metrics = {
            'loss': total_loss / num_batches,
            'ce_loss': total_ce_loss / num_batches,
            'constraint_loss': total_constraint_loss / num_batches,
            'accuracy': 100.0 * correct_cells / total_cells if total_cells > 0 else 0.0,
            'epoch_time': epoch_time,
            'samples_per_sec': len(train_loader.dataset) / epoch_time
        }
        
        return metrics
    
    @torch.no_grad()
    def validate(
        self,
        val_loader,
        desc: str = "Validation",
        use_ema: bool = True
    ) -> Dict[str, float]:
        """Validate model with enhanced metrics.
        
        Args:
            val_loader: Validation data loader
            desc: Progress bar description
            use_ema: Use EMA weights if available
            
        Returns:
            Dictionary with validation metrics
        """
        # Use EMA model for validation if available
        model_to_eval = self.ema.shadow if (use_ema and self.ema) else self.model
        model_to_eval.eval()
        
        total_loss = 0.0
        correct_cells = 0
        correct_grids = 0
        total_cells = 0
        total_grids = 0
        
        all_solve_rates = []
        all_confidences = []
        all_entropies = []
        all_violations = []
        
        pbar = tqdm(val_loader, desc=desc)
        for puzzles, solutions in pbar:
            puzzles = puzzles.to(self.device)
            solutions = solutions.to(self.device)
            
            # Forward pass
            if self.use_amp:
                with autocast(device_type='cuda', dtype=torch.float16):
                    logits = model_to_eval(puzzles)
                    loss, loss_info = self.criterion(logits, solutions, puzzles)
            else:
                logits = model_to_eval(puzzles)
                loss, loss_info = self.criterion(logits, solutions, puzzles)
            
            total_loss += loss_info['total_loss']
            
            # Compute accuracy
            mask = (puzzles == 0)
            preds = logits.argmax(dim=-1) + 1
            
            for i in range(len(puzzles)):
                if mask[i].any():
                    correct = (preds[i][mask[i]] == solutions[i][mask[i]]).sum().item()
                    total = mask[i].sum().item()
                    correct_cells += correct
                    total_cells += total
                    
                    if correct == total:
                        correct_grids += 1
                
                total_grids += 1
            
            # Advanced metrics
            adv_metrics = self._compute_advanced_metrics(logits, solutions, puzzles)
            all_solve_rates.append(adv_metrics['solve_rate'])
            all_confidences.append(adv_metrics['confidence'])
            all_entropies.append(adv_metrics['entropy'])
            all_violations.append(adv_metrics['violations'])
        
        # Average metrics
        metrics = {
            'loss': total_loss / len(val_loader),
            'cell_accuracy': 100.0 * correct_cells / total_cells if total_cells > 0 else 0.0,
            'grid_accuracy': 100.0 * correct_grids / total_grids if total_grids > 0 else 0.0,
            'solve_rate': 100.0 * np.mean(all_solve_rates),
            'confidence': np.mean(all_confidences),
            'entropy': np.mean(all_entropies),
            'avg_violations': np.mean(all_violations)
        }
        
        return metrics
    
    def train(
        self,
        puzzles: torch.Tensor,
        solutions: torch.Tensor,
        num_epochs: int = 60,
        batch_size: int = 256,
        val_split: float = 0.1,
        curriculum_epochs: List[int] = [20, 20, 20],
        augment: bool = True,
        num_workers: int = 8,
        prefetch_factor: int = 2,
        persistent_workers: bool = True
    ):
        """Full training loop with all production features.
        
        Args:
            puzzles: Training puzzles
            solutions: Training solutions
            num_epochs: Total epochs (overridden by curriculum_epochs sum)
            batch_size: Batch size per GPU
            val_split: Validation split ratio
            curriculum_epochs: Epochs per stage [easy, medium, hard]
            augment: Apply data augmentation
            num_workers: DataLoader workers
            prefetch_factor: Prefetch batches per worker
            persistent_workers: Keep workers alive between epochs
        """
        total_epochs = sum(curriculum_epochs)
        logger.info(f"Starting production training for {total_epochs} epochs")
        logger.info(f"Device: {self.device}, Mixed Precision: {self.use_amp}")
        logger.info(f"Effective batch size: {batch_size} Ã— {self.accumulation_steps} = {batch_size * self.accumulation_steps}")
        logger.info(f"Curriculum stages: {curriculum_epochs}")
        logger.info(f"DataLoader: workers={num_workers}, prefetch={prefetch_factor}, persistent={persistent_workers}")
        
        # Initialize scheduler
        if self.scheduler_type == 'onecycle':
            # We'll create this per-stage due to curriculum learning
            scheduler = None
            logger.info(f"Using OneCycleLR scheduler with max_lr={self.max_lr}")
        else:
            scheduler = CosineAnnealingLR(
                self.optimizer,
                T_max=total_epochs - self.warmup_epochs,
                eta_min=1e-6
            )
            logger.info("Using CosineAnnealingLR scheduler")
        
        for stage_idx, stage_epochs in enumerate(curriculum_epochs):
            logger.info(f"\n{'='*70}")
            logger.info(f"Curriculum Stage {stage_idx} ({['Easy', 'Medium', 'Hard'][stage_idx]})")
            logger.info(f"{'='*70}")
            
            self.curriculum_stage = stage_idx
            
            # Create dataloaders with optimized settings
            train_loader, val_loader = create_curriculum_dataloaders(
                puzzles,
                solutions,
                batch_size=batch_size,
                val_split=val_split,
                curriculum_stage=stage_idx,
                augment=augment,
                num_workers=num_workers,
                # Additional optimizations
                pin_memory=True,
                prefetch_factor=prefetch_factor if num_workers > 0 else None,
                persistent_workers=persistent_workers if num_workers > 0 else False
            )
            
            # Create OneCycleLR for this stage if needed
            if self.scheduler_type == 'onecycle' and scheduler is None:
                scheduler = OneCycleLR(
                    self.optimizer,
                    max_lr=self.max_lr,
                    epochs=total_epochs,
                    steps_per_epoch=len(train_loader) // self.accumulation_steps,
                    pct_start=0.3,
                    anneal_strategy='cos'
                )
            
            # Train for stage epochs
            for epoch in range(stage_epochs):
                self.current_epoch += 1
                
                # Learning rate warmup
                if self.current_epoch <= self.warmup_epochs:
                    warmup_progress = self.current_epoch / self.warmup_epochs
                    lr = self.base_lr * (self.warmup_factor + (1 - self.warmup_factor) * warmup_progress)
                    for param_group in self.optimizer.param_groups:
                        param_group['lr'] = lr
                    logger.info(f"\nEpoch {self.current_epoch}/{total_epochs} [WARMUP] - LR: {lr:.6f}")
                else:
                    current_lr = self.optimizer.param_groups[0]['lr']
                    logger.info(f"\nEpoch {self.current_epoch}/{total_epochs} - LR: {current_lr:.6f}")
                
                # Train
                train_metrics = self.train_epoch(
                    train_loader,
                    desc=f"Epoch {self.current_epoch} - Stage {stage_idx} - Training"
                )
                
                logger.info(
                    f"Train - Loss: {train_metrics['loss']:.4f}, "
                    f"CE: {train_metrics['ce_loss']:.4f}, "
                    f"Constraint: {train_metrics['constraint_loss']:.4f}, "
                    f"Acc: {train_metrics['accuracy']:.2f}%, "
                    f"Time: {train_metrics['epoch_time']:.1f}s, "
                    f"Speed: {train_metrics['samples_per_sec']:.0f} samples/s"
                )
                
                # Validate (adaptive frequency)
                val_metrics = None
                if self._should_validate(self.current_epoch):
                    val_metrics = self.validate(
                        val_loader,
                        desc=f"Epoch {self.current_epoch} - Validation",
                        use_ema=True
                    )
                    
                    logger.info(
                        f"Val - Loss: {val_metrics['loss']:.4f}, "
                        f"Cell Acc: {val_metrics['cell_accuracy']:.2f}%, "
                        f"Grid Acc: {val_metrics['grid_accuracy']:.2f}%, "
                        f"Solve Rate: {val_metrics['solve_rate']:.2f}%"
                    )
                    logger.info(
                        f"Val - Confidence: {val_metrics['confidence']:.4f}, "
                        f"Entropy: {val_metrics['entropy']:.4f}, "
                        f"Violations: {val_metrics['avg_violations']:.2f}"
                    )
                    
                    # Update history
                    self.training_history['val_loss'].append(val_metrics['loss'])
                    self.training_history['val_cell_acc'].append(val_metrics['cell_accuracy'])
                    self.training_history['val_grid_acc'].append(val_metrics['grid_accuracy'])
                    self.training_history['val_solve_rate'].append(val_metrics['solve_rate'])
                
                # Update scheduler
                if self.current_epoch > self.warmup_epochs and scheduler:
                    if self.scheduler_type == 'onecycle':
                        # OneCycleLR steps per batch
                        pass  # Already stepped in train_epoch
                    else:
                        scheduler.step()
                
                # Update history
                self.training_history['train_loss'].append(train_metrics['loss'])
                self.training_history['train_acc'].append(train_metrics['accuracy'])
                self.training_history['lr'].append(self.optimizer.param_groups[0]['lr'])
                
                # Save checkpoint
                if val_metrics:
                    is_best = val_metrics['grid_accuracy'] > self.best_val_acc
                    self.save_checkpoint(
                        epoch=self.current_epoch,
                        metrics=val_metrics,
                        is_best=is_best
                    )
                    
                    if is_best:
                        self.best_val_acc = val_metrics['grid_accuracy']
                        logger.info(f"âœ¨ New best grid accuracy: {self.best_val_acc:.2f}%")
                    
                    # Early stopping
                    if self.early_stopping(val_metrics['grid_accuracy']):
                        logger.info(f"\nðŸ›‘ Early stopping triggered at epoch {self.current_epoch}")
                        logger.info(f"Best grid accuracy: {self.best_val_acc:.2f}%")
                        logger.info(f"No improvement for {self.early_stopping.patience} epochs")
                        return
        
        logger.info(f"\n{'='*70}")
        logger.info(f"ðŸŽ‰ Training completed!")
        logger.info(f"Best validation grid accuracy: {self.best_val_acc:.2f}%")
        logger.info(f"Total epochs: {self.current_epoch}")
        logger.info(f"{'='*70}")
    
    def save_checkpoint(
        self,
        epoch: int,
        metrics: Dict[str, float],
        is_best: bool = False
    ):
        """Save comprehensive checkpoint."""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'metrics': metrics,
            'best_val_acc': self.best_val_acc,
            'curriculum_stage': self.curriculum_stage,
            'training_history': self.training_history
        }
        
        if self.scaler:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        if self.ema:
            checkpoint['ema_state_dict'] = self.ema.state_dict()
        
        # Save latest
        torch.save(checkpoint, self.checkpoint_dir / 'policy.pt')
        
        # Save best
        if is_best:
            torch.save(checkpoint, self.checkpoint_dir / 'policy_best.pt')
            
            # Also save EMA model separately for inference
            if self.ema:
                torch.save(self.ema.shadow.state_dict(), self.checkpoint_dir / 'policy_ema.pt')
            
            logger.info(f"ðŸ’¾ Saved best checkpoint at epoch {epoch}")
    
    def load_checkpoint(self, checkpoint_path: str | Path):
        """Load comprehensive checkpoint."""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.best_val_acc = checkpoint.get('best_val_acc', 0.0)
        self.current_epoch = checkpoint.get('epoch', 0)
        self.curriculum_stage = checkpoint.get('curriculum_stage', 0)
        self.training_history = checkpoint.get('training_history', self.training_history)
        
        if self.scaler and 'scaler_state_dict' in checkpoint:
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
        
        if self.ema and 'ema_state_dict' in checkpoint:
            self.ema.load_state_dict(checkpoint['ema_state_dict'])
        
        logger.info(f"Loaded checkpoint from epoch {self.current_epoch}")
        logger.info(f"Best validation accuracy: {self.best_val_acc:.2f}%")
