{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5278223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Configuration and Imports\n",
    "import os, sys, shutil, subprocess, json, textwrap, platform, time, hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect Kaggle paths vs local\n",
    "IN_KAGGLE = Path('/kaggle/working').exists()\n",
    "WORK_DIR = Path('/kaggle/working') if IN_KAGGLE else Path.cwd() / 'working'\n",
    "REPO_NAME = 'sudoku-ai'\n",
    "REPO_URL = 'https://github.com/NIKHILSAI71/sudoku-ai.git'\n",
    "REPO_PATH = WORK_DIR / REPO_NAME\n",
    "EXPORT_DIR = REPO_PATH / 'export'\n",
    "OUTPUTS_DIR = REPO_PATH / 'outputs'\n",
    "(WORK_DIR).mkdir(parents=True, exist_ok=True)\n",
    "(OUTPUTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def run(cmd, cwd=None, env=None, check=True):\n",
    "    print('>>', ' '.join(cmd))\n",
    "    return subprocess.run(cmd, cwd=cwd, env=env, check=check)\n",
    "\n",
    "def print_tree(path: Path, depth: int = 2):\n",
    "    path = Path(path)\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        level = Path(root).relative_to(path).parts\n",
    "        if len(level) > depth:\n",
    "            continue\n",
    "        indent = '  ' * len(level)\n",
    "        for d in dirs:\n",
    "            print(f\"{indent}{d}/\")\n",
    "        for f in files:\n",
    "            print(f\"{indent}{f}\")\n",
    "\n",
    "print('IN_KAGGLE:', IN_KAGGLE)\n",
    "print('WORK_DIR:', WORK_DIR)\n",
    "print('REPO_PATH:', REPO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f844581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and Clone Repository from GitHub\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "# --- Configuration ---\n",
    "working_directory = str(WORK_DIR) + '/' if not str(WORK_DIR).endswith('/') else str(WORK_DIR)\n",
    "repo_name = REPO_NAME\n",
    "repo_path = os.path.join(working_directory, repo_name)\n",
    "repo_url = REPO_URL\n",
    "\n",
    "print('--- Starting Setup ---')\n",
    "if not os.path.exists(working_directory):\n",
    "    os.makedirs(working_directory)\n",
    "    print(f'Created working directory: {working_directory}')\n",
    "\n",
    "if os.path.exists(repo_path):\n",
    "    print(f'Found existing directory at {repo_path}. Removing it.')\n",
    "    try:\n",
    "        shutil.rmtree(repo_path)\n",
    "        print('Successfully removed the old directory.')\n",
    "    except OSError as e:\n",
    "        print(f'Error removing directory {repo_path}: {e}')\n",
    "else:\n",
    "    print(f'No existing directory found at {repo_path}. Ready to clone.')\n",
    "\n",
    "os.chdir(working_directory)\n",
    "print(f'Changed current directory to: {os.getcwd()}')\n",
    "\n",
    "print(f'Cloning repository from {repo_url}...')\n",
    "try:\n",
    "    subprocess.run(['git','clone', repo_url], check=True)\n",
    "    print('Repository cloned successfully!')\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f'Failed to clone repository: {e}')\n",
    "\n",
    "print('\\n--- Verification ---')\n",
    "if os.path.exists(repo_path):\n",
    "    print(f\"Success! The directory '{repo_name}' is now in {working_directory}.\")\n",
    "    print('Contents:')\n",
    "    for item in os.listdir(repo_path):\n",
    "        print(f'- {item}')\n",
    "else:\n",
    "    print(f\"Verification failed. The directory '{repo_name}' was not created.\")\n",
    "print('\\n--- Setup Complete ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bfbc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Clone and List Files\n",
    "from pathlib import Path\n",
    "root = Path(repo_path)\n",
    "assert root.exists(), f'Clone missing at {root}'\n",
    "print('Top-level:')\n",
    "for p in sorted(root.iterdir()):\n",
    "    print('-', p.name)\n",
    "print('\\nKey files present:')\n",
    "for name in ['pyproject.toml','README.md','cli','sudoku_ai','sudoku_engine','tests']:\n",
    "    print(name, '->', (root / name).exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937809aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and Activate Virtual Environment\n",
    "import sys\n",
    "VENV_PATH = Path(repo_path) / '.venv'\n",
    "# Always prefer running pip via the Python interpreter to avoid missing pip3 paths\n",
    "if not IN_KAGGLE:\n",
    "    run([sys.executable, '-m', 'venv', str(VENV_PATH)])\n",
    "    PY = str(VENV_PATH / ('Scripts/python.exe' if platform.system()=='Windows' else 'bin/python'))\n",
    "    PIP = PY  # we'll call with -m pip\n",
    "else:\n",
    "    # In Kaggle, use current interpreter for -m pip\n",
    "    PY = sys.executable\n",
    "    PIP = PY  # we'll call with -m pip\n",
    "print('Using python:', PY)\n",
    "print('pip will be invoked as: python -m pip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75a50ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Project Dependencies\n",
    "req = Path(repo_path) / 'requirements.txt'\n",
    "pyproject = Path(repo_path) / 'pyproject.toml'\n",
    "if req.exists():\n",
    "    run([PY, '-m', 'pip', 'install', '-r', str(req)])\n",
    "else:\n",
    "    # Install project in editable mode\n",
    "    run([PY, '-m', 'pip', 'install', '-U', 'pip', 'setuptools', 'wheel'])\n",
    "    run([PY, '-m', 'pip', 'install', '-e', str(REPO_PATH)], cwd=str(REPO_PATH))\n",
    "# Optional extras: sat and cpsat backends (skip if wheels unavailable)\n",
    "try:\n",
    "    run([PY, '-m', 'pip', 'install', 'python-sat[pblib,aiger]>=0.1.8'])\n",
    "except Exception as e:\n",
    "    print('python-sat optional install failed:', e)\n",
    "try:\n",
    "    run([PY, '-m', 'pip', 'install', 'ortools>=9.10'])\n",
    "except Exception as e:\n",
    "    print('ortools optional install failed:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06be126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Setup Tasks\n",
    "(REPO_PATH / 'checkpoints').mkdir(parents=True, exist_ok=True)\n",
    "(REPO_PATH / 'logs').mkdir(parents=True, exist_ok=True)\n",
    "(EXPORT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "(OUTPUTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "# Copy sample puzzle to outputs for quick test\n",
    "shutil.copy(REPO_PATH / 'examples' / 'easy1.sdk', OUTPUTS_DIR / 'easy1.sdk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cba842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover Training and Testing Entrypoints\n",
    "# The project exposes training via cli ai-solve --train or via sudoku_ai.policy.train_supervised\n",
    "print('CLI entry:', 'python -m cli.main ai-solve --help')\n",
    "print('Module entry:', 'from sudoku_ai.policy import train_supervised')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e462767",
   "metadata": {},
   "source": [
    "## Prepare dataset from Kaggle CSV\n",
    "\n",
    "This step converts the Kaggle `sudoku.csv` into three files in the working directory:\n",
    "- `data.jsonl` (with keys `puzzle` and `solution`)\n",
    "- `puzzles.txt`\n",
    "- `solutions.txt`\n",
    "\n",
    "Make sure the dataset is attached in Kaggle under the Data tab and the CSV path is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c6b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the input and output file paths\n",
    "# IMPORTANT: Double-check this path in your Kaggle notebook's right-hand sidebar under \"Data\".\n",
    "# You can use the \"Copy file path\" button to ensure it's correct.\n",
    "input_file = '/kaggle/input/sudoku/sudoku.csv'\n",
    "jsonl_output_file = 'data.jsonl'\n",
    "puzzles_output_file = 'puzzles.txt'\n",
    "solutions_output_file = 'solutions.txt'\n",
    "\n",
    "# Use a chunk size to process the large file efficiently\n",
    "chunk_size = 100_000\n",
    "\n",
    "print(\"Starting data processing...\")\n",
    "\n",
    "try:\n",
    "    # Use an iterator to read the CSV file in chunks to save memory\n",
    "    reader = pd.read_csv(input_file, chunksize=chunk_size, iterator=True)\n",
    "\n",
    "    # Open all output files at once. 'w' mode will create/overwrite them.\n",
    "    with open(jsonl_output_file, 'w') as jsonl_file, \\\n",
    "         open(puzzles_output_file, 'w') as puzzles_file, \\\n",
    "         open(solutions_output_file, 'w') as solutions_file:\n",
    "\n",
    "        # Loop through each chunk from the CSV reader\n",
    "        for i, chunk_df in enumerate(reader):\n",
    "            # --- 1. Process and write the JSONL data ---\n",
    "\n",
    "            # Rename columns to match the desired JSON keys ('puzzle', 'solution')\n",
    "            json_chunk = chunk_df.rename(columns={'quizzes': 'puzzle', 'solutions': 'solution'})\n",
    "\n",
    "            # Convert the entire DataFrame chunk to a JSONL string in one go.\n",
    "            # orient='records' and lines=True creates one JSON object per line.\n",
    "            # This is much faster than iterating row by row.\n",
    "            jsonl_string = json_chunk[['puzzle', 'solution']].to_json(orient='records', lines=True)\n",
    "\n",
    "            # Write the generated string to the .jsonl file\n",
    "            jsonl_file.write(jsonl_string + \"\\n\")\n",
    "\n",
    "            # --- 2. Process and write the plain text files ---\n",
    "\n",
    "            # For puzzles.txt, join all quizzes in the chunk with a newline character.\n",
    "            # Add a final newline to ensure proper line separation between chunks.\n",
    "            puzzles_text = '\\n'.join(chunk_df['quizzes'].astype(str)) + '\\n'\n",
    "            puzzles_file.write(puzzles_text)\n",
    "\n",
    "            # Do the same for solutions.txt\n",
    "            solutions_text = '\\n'.join(chunk_df['solutions'].astype(str)) + '\\n'\n",
    "            solutions_file.write(solutions_text)\n",
    "\n",
    "            # Print progress to the console\n",
    "            print(f\"Processed chunk {i + 1}...\")\n",
    "\n",
    "    print(f\"\\nSuccessfully created {jsonl_output_file}, {puzzles_output_file}, and {solutions_output_file}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at '{input_file}'.\")\n",
    "    print(\"Please verify the dataset path in your Kaggle notebook's data section.\")\n",
    "    print(\"Click the 'Copy file path' button next to the CSV file to get the correct path.\")\n",
    "except KeyError:\n",
    "    # This block runs if the columns 'quizzes' or 'solutions' don't exist\n",
    "    try:\n",
    "        # Try to read the first few lines to see what the actual column names are\n",
    "        sample_df = pd.read_csv(input_file, nrows=5)\n",
    "        print(\"Error: The CSV file does not contain 'quizzes' or 'solutions' columns.\")\n",
    "        print(f\"The available columns are: {list(sample_df.columns)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while diagnosing the KeyError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f113d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-parameter training via CLI\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure the dataset exists\n",
    "assert Path('data.jsonl').exists(), 'data.jsonl not found. Run the dataset prep cell first.'\n",
    "\n",
    "# Use the Python interpreter resolved earlier (PY) if available; otherwise fallback to sys.executable\n",
    "try:\n",
    "    PY\n",
    "except NameError:\n",
    "    PY = sys.executable\n",
    "\n",
    "# Train with high parameters\n",
    "from subprocess import CalledProcessError\n",
    "print('Starting high-parameter training...')\n",
    "args = [\n",
    "    PY, '-m', 'cli.main', 'ai-solve', '--train',\n",
    "    '--dataset', 'data.jsonl',\n",
    "    '--ckpt', 'checkpoints/policy.pt',\n",
    "    '--train-epochs', '500',\n",
    "    '--train-limit', '20000'\n",
    "]\n",
    "print('>>', ' '.join(args))\n",
    "try:\n",
    "    import subprocess\n",
    "    subprocess.run(args, check=True)\n",
    "except CalledProcessError as e:\n",
    "    print('Training failed with exit code', e.returncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5a990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Training Job (production-style)\n",
    "code = r'''\n",
    "import json, os, sys, time\n",
    "from pathlib import Path\n",
    "from sudoku_ai.policy import train_supervised\n",
    "\n",
    "ckpt = Path('checkpoints/policy.pt')\n",
    "ckpt.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Prefer a prepared dataset if available, otherwise fall back to a tiny puzzles file\n",
    "dataset_jsonl = Path('data.jsonl') if Path('data.jsonl').exists() else None\n",
    "puzzles_path = None\n",
    "solutions_path = None\n",
    "if dataset_jsonl is None:\n",
    "    # Create a tiny puzzles file from example to bootstrap training\n",
    "    puzzles_path = Path('outputs/_puzzles.txt')\n",
    "    puzzles_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    puzzle = Path('examples/easy1.sdk').read_text().strip()\n",
    "    puzzles_path.write_text(puzzle + '\\n')\n",
    "    print('Training from example puzzle at:', puzzles_path)\n",
    "else:\n",
    "    print('Training from dataset:', dataset_jsonl)\n",
    "\n",
    "hist = train_supervised(\n",
    "    out_path=str(ckpt),\n",
    "    dataset_jsonl=str(dataset_jsonl) if dataset_jsonl else None,\n",
    "    puzzles_path=str(puzzles_path) if puzzles_path else None,\n",
    "    solutions_path=str(solutions_path) if solutions_path else None,\n",
    "    epochs=500,\n",
    "    batch_size=64,\n",
    "    lr=3e-4,\n",
    "    val_split=0.1,\n",
    "    max_samples=20000,\n",
    "    augment=True,\n",
    "    amp=False,\n",
    "    seed=42,\n",
    "    overfit=False,\n",
    ")\n",
    "print('Saved checkpoint ->', ckpt)\n",
    "print('Best epoch:', hist.get('best_epoch'))\n",
    "'''\n",
    "run([PY, '-c', code], cwd=str(REPO_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e15e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Evaluation/Tests\n",
    "import json, time\n",
    "metrics_path = REPO_PATH / 'export' / 'metrics.json'\n",
    "(REPO_PATH / 'export').mkdir(parents=True, exist_ok=True)\n",
    "start = time.time()\n",
    "# Run a very small subset: engine tests and heuristics tests can be heavy; use -q\n",
    "try:\n",
    "    run([PY, '-m', 'pytest', '-q', 'tests/test_engine.py', 'tests/test_solvers.py'], cwd=str(REPO_PATH), check=False)\n",
    "except Exception as e:\n",
    "    print('pytest encountered issues:', e)\n",
    "# Sanity: run CLI ai-solve with the produced checkpoint\n",
    "try:\n",
    "    run([PY, '-m', 'cli.main', 'ai-solve', '-i', 'examples/easy1.sdk', '--ckpt', 'checkpoints/policy.pt', '--max-steps', '50'], cwd=str(REPO_PATH), check=False)\n",
    "except Exception as e:\n",
    "    print('cli ai-solve failed:', e)\n",
    "dur = time.time() - start\n",
    "with open(metrics_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump({'duration_sec': dur, 'notes': 'Light tests on Kaggle'}, f, indent=2)\n",
    "print('Wrote metrics ->', metrics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76588d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Trained Artifacts\n",
    "from datetime import datetime\n",
    "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ckpt_src = REPO_PATH / 'checkpoints' / 'policy.pt'\n",
    "ckpt_dst = EXPORT_DIR / 'policy.pt'\n",
    "if ckpt_src.exists():\n",
    "    shutil.copy(ckpt_src, ckpt_dst)\n",
    "else:\n",
    "    raise FileNotFoundError('Checkpoint not found at ' + str(ckpt_src))\n",
    "# Minimal config\n",
    "config = {\n",
    "    'model': 'ResNetPolicy',\n",
    "    'created': datetime.utcnow().isoformat() + 'Z',\n",
    "    'repo': REPO_URL,\n",
    "}\n",
    "(EXPORT_DIR / 'config.json').write_text(json.dumps(config, indent=2), encoding='utf-8')\n",
    "# Model card\n",
    "model_card = f'''\n",
    "---\n",
    "license: mit\n",
    "library_name: pytorch\n",
    "tags: [sudoku, policy, deep-learning, gradio]\n",
    "---\n",
    ",\n",
    "# Sudoku AI Policy\n",
    "This repository hosts a lightweight Sudoku policy network and a Gradio demo.\n",
    "Artifacts were prepared from {REPO_URL}.\n",
    "'''.replace(' ,', '')\n",
    "(EXPORT_DIR / 'README.md').write_text(model_card, encoding='utf-8')\n",
    "print('Export contents:')\n",
    "for p in sorted(EXPORT_DIR.iterdir()):\n",
    "    print('-', p.name, p.stat().st_size, 'bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b434726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate to Hugging Face Hub\n",
    "import os\n",
    "try:\n",
    "    from huggingface_hub import notebook_login, whoami\n",
    "except ImportError:\n",
    "    run([PY, '-m', 'pip', 'install', '-U', 'huggingface_hub'])\n",
    "    from huggingface_hub import notebook_login, whoami\n",
    "HF_TOKEN = os.environ.get('HUGGINGFACEHUB_API_TOKEN') or os.environ.get('HF_TOKEN')\n",
    "if HF_TOKEN:\n",
    "    os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "    print('Using token from environment')\n",
    "else:\n",
    "    print('Please login interactively:')\n",
    "    notebook_login()\n",
    "try:\n",
    "    print('whoami:', whoami())\n",
    "except Exception as e:\n",
    "    print('whoami failed:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205beaa8",
   "metadata": {},
   "source": [
    "## Authenticate to Hugging Face Hub\n",
    "\n",
    "Use environment variable `HF_TOKEN`/`HUGGINGFACEHUB_API_TOKEN` or login interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/Use Hub Repository and Push Model Artifacts\n",
    "try:\n",
    "    from huggingface_hub import HfApi, create_repo, upload_folder\n",
    "except ImportError:\n",
    "    run([PY, '-m', 'pip', 'install', '-U', 'huggingface_hub'])\n",
    "    from huggingface_hub import HfApi, create_repo, upload_folder\n",
    "api = HfApi()\n",
    "hub_model_id = 'nikhilsaipagidimarri/sudoku-ai'\n",
    "create_repo(hub_model_id, exist_ok=True, repo_type='model')\n",
    "print('Pushing model artifacts...')\n",
    "upload_folder(\n",
    "    repo_id=hub_model_id,\n",
    "    folder_path=str(EXPORT_DIR),\n",
    "    repo_type='model',\n",
    "    commit_message='Add trained checkpoint, config, and metrics'\n",
    ")\n",
    "print('Model pushed to https://huggingface.co/' + hub_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gradio Space and push interface\n",
    "try:\n",
    "    from huggingface_hub import create_repo, upload_file\n",
    "except ImportError:\n",
    "    run([PY, '-m', 'pip', 'install', '-U', 'huggingface_hub'])\n",
    "    from huggingface_hub import create_repo, upload_file\n",
    "space_id = 'nikhilsaipagidimarri/sudoku-ai-demo'\n",
    "create_repo(space_id, repo_type='space', exist_ok=True, space_sdk='gradio')\n",
    "SPACE_DIR = WORK_DIR / 'space_work'\n",
    "SPACE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# requirements for space\n",
    "(SPACE_DIR / 'requirements.txt').write_text('\\n'.join(['torch','numpy','rich','gradio']), encoding='utf-8')\n",
    "app_py = f'''\n",
    "import gradio as gr\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from sudoku_ai.policy import load_policy, board_to_tensor\n",
    "from sudoku_engine import parse_line, board_to_line\n",
    "from sudoku_engine.board import Board\n",
    "\n",
    "CKPT = 'policy.pt'\n",
    "def load():\n",
    "    return load_policy(CKPT)\n",
    "\n",
    "policy = load()\n",
    "\n",
    "def solve_sudoku(puzzle_line: str):\n",
    "    line = ''.join(ch for ch in puzzle_line if ch.isdigit())\n",
    "    if len(line) != 81:\n",
    "        return 'Input must be 81 digits.'\n",
    "    b = Board(parse_line(line))\n",
    "    # Simple greedy decode with policy probabilities and legality mask\n",
    "    for _ in range(200):\n",
    "        if b.is_complete():\n",
    "            break\n",
    "        x = board_to_tensor(board_to_line(b.grid)).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits = policy(x)[0]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "        masks = b.candidates_mask()\n",
    "        mask_tensor = torch.zeros(81, 9)\n",
    "        for idx in range(81):\n",
    "            r, c = divmod(idx, 9)\n",
    "            m = int(masks[r, c])\n",
    "            if b.grid[r, c] != 0 or m == 0:\n",
    "                continue\n",
    "            for d in range(1, 10):\n",
    "                if m & (1 << (d - 1)):\n",
    "                    mask_tensor[idx, d - 1] = 1.0\n",
    "        masked = probs * mask_tensor\n",
    "        flat = masked.view(-1)\n",
    "        if float(flat.sum().item()) <= 0.0:\n",
    "            break\n",
    "        choice = int(torch.argmax(flat).item())\n",
    "        cell, dig_idx = divmod(choice, 9)\n",
    "        r, c = divmod(cell, 9)\n",
    "        d = dig_idx + 1\n",
    "        b.set_cell(r, c, d)\n",
    "    return board_to_line(b.grid)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=solve_sudoku,\n",
    "    inputs=gr.Textbox(label='Enter Sudoku (81 digits, 0=empty)', lines=2),\n",
    "    outputs=gr.Textbox(label='Solved/Final Board'),\n",
    "    title='Sudoku AI (Policy)',\n",
    "    description='Paste an 81-digit Sudoku string. Uses policy checkpoint and legality mask.'\n",
    ")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo.launch()\n",
    "'''\n",
    "(SPACE_DIR / 'app.py').write_text(app_py, encoding='utf-8')\n",
    "# copy checkpoint and minimal packages of project files used by interface\n",
    "shutil.copy(EXPORT_DIR / 'policy.pt', SPACE_DIR / 'policy.pt')\n",
    "# Upload space files\n",
    "upload_file(path_or_fileobj=str(SPACE_DIR / 'app.py'), path_in_repo='app.py', repo_id=space_id, repo_type='space')\n",
    "upload_file(path_or_fileobj=str(SPACE_DIR / 'requirements.txt'), path_in_repo='requirements.txt', repo_id=space_id, repo_type='space')\n",
    "upload_file(path_or_fileobj=str(SPACE_DIR / 'policy.pt'), path_in_repo='policy.pt', repo_id=space_id, repo_type='space')\n",
    "print('Space pushed: https://huggingface.co/spaces/' + space_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e207d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record Run Metadata\n",
    "import sys, subprocess, hashlib, time\n",
    "run_meta = {\n",
    "    'python': sys.version,\n",
    "    'platform': platform.platform(),\n",
    "    'time_utc': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),\n",
    "    'repo_url': REPO_URL,\n",
    "    'work_dir': str(WORK_DIR),\n",
    "}\n",
    "try:\n",
    "    out = subprocess.check_output(['git', '-C', str(REPO_PATH), 'rev-parse', 'HEAD']).decode().strip()\n",
    "    run_meta['git_commit'] = out\n",
    "except Exception:\n",
    "    run_meta['git_commit'] = None\n",
    "# hash checkpoint\n",
    "def sha256(p: Path):\n",
    "    h = hashlib.sha256()\n",
    "    with open(p, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b''):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "run_meta['artifacts'] = {}\n",
    "for p in [EXPORT_DIR / 'policy.pt', EXPORT_DIR / 'config.json', EXPORT_DIR / 'README.md']:\n",
    "    if p.exists():\n",
    "        run_meta['artifacts'][p.name] = {'bytes': p.stat().st_size, 'sha256': sha256(p)}\n",
    "(EXPORT_DIR / 'run.json').write_text(json.dumps(run_meta, indent=2), encoding='utf-8')\n",
    "print('Saved run.json ->', EXPORT_DIR / 'run.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22fb75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install deps (Kaggle-friendly) and add repo to sys.path\n",
    "import sys\n",
    "# Base tools and libs we need regardless of package install\n",
    "base_pkgs = ['pip', 'setuptools', 'wheel', 'numpy', 'huggingface_hub', 'gradio', 'pytest']\n",
    "run([PY, '-m', 'pip', 'install', '-U'] + base_pkgs)\n",
    "if IN_KAGGLE:\n",
    "    # Avoid pip -e . due to Python>=3.11 constraint; use path insert instead\n",
    "    if str(REPO_PATH) not in sys.path:\n",
    "        sys.path.insert(0, str(REPO_PATH))\n",
    "    print('Added to sys.path:', REPO_PATH)\n",
    "else:\n",
    "    # On local py>=3.11, install the package for scripts entry points\n",
    "    run([PY, '-m', 'pip', 'install', '-e', str(REPO_PATH)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8800e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vendor minimal project modules into Space so imports work\n",
    "import glob\n",
    "for pkg in ['sudoku_ai','sudoku_engine']:\n",
    "    src = REPO_PATH / pkg\n",
    "    dst = SPACE_DIR / pkg\n",
    "    if dst.exists():\n",
    "        shutil.rmtree(dst)\n",
    "    shutil.copytree(src, dst)\n",
    "    print('Copied', pkg, '->', dst)\n",
    "# upload folders to space (recursive)\n",
    "try:\n",
    "    from huggingface_hub import upload_folder\n",
    "except ImportError:\n",
    "    run([PY, '-m', 'pip', 'install', '-U', 'huggingface_hub'])\n",
    "    from huggingface_hub import upload_folder\n",
    "upload_folder(repo_id=space_id, repo_type='space', folder_path=str(SPACE_DIR), commit_message='Update app with vendored modules and checkpoint')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
